# AI Agents Guide for PyDynamicReporting

This guide helps AI coding agents make high-quality, repo-aligned changes to **PyDynamicReporting**, which is the Python client library for Ansys Dynamic Reporting.
It focuses on *how this repository is wired* (tooling, structure, conventions) so agents can work quickly without breaking CI.
Read through it carefully before making changes. Read the README, especially the development setup section, for additional context.

## Quick agent contract (what “done” means)

When you change code in this repo, aim to always:

- **Preserve public APIs** unless the change explicitly requires a breaking change.
- **Add or update tests** for behavior changes (at least: happy path + 1 edge case).
- **Run the repo’s quality gates** before concluding:
  - `make check` (lockfile consistency + pre-commit)
  - `make test` (pytest + coverage)
  - `make smoketest` (fast import sanity check)
- **Avoid unrelated reformatting** and keep diffs tight.

## Project overview

**PyDynamicReporting** is the Python client library for **Ansys Dynamic Reporting** (formerly documented as Nexus). It provides a Pythonic interface for pushing items (images, text, 3D scenes, tables, etc.) into a reporting database and creating dynamic reports.

- **Repository**: https://github.com/ansys/pydynamicreporting
- **Documentation**: https://dynamicreporting.docs.pyansys.com/
- **Package**: `ansys-dynamicreporting-core` on PyPI
- **Python support**: 3.10–3.12 (`requires-python = ">=3.10, <3.13"`)
- **License**: MIT

## Repository structure (high level)

```
├── src/ansys/dynamicreporting/core/    # Main package
│   ├── adr_item.py                     # Item management (generated in some workflows)
│   ├── adr_report.py                   # Report objects
│   ├── adr_service.py                  # Service interface
│   ├── docker_support.py               # Docker integration
│   ├── serverless/                     # Serverless implementation
│   └── utils/                          # Additional utilities
├── tests/                              # Pytest suite
├── doc/                                # Sphinx docs
├── codegen/                            # Code-generation/build hooks
├── default_templates/                  # Report templates
└── scripts/                            # Dev/release helper scripts
```

Notes:
- Some files (notably `adr_item.py`, `adr_utils.py`, and `build_info.py`) may be **generated** via build hooks / codegen. Avoid hand-editing those unless you’re intentionally changing generation inputs.

## Development setup (repo-native)

This repo uses **uv** + a checked-in lockfile (`uv.lock`). The Makefile wraps the canonical workflows.

### Prerequisites
- Python 3.10, 3.11, or 3.12
- `uv`
- `make` (on Windows, `make install` uses Git Bash under the hood)

### Install (preferred)

```bash
make install
```

What it does:
- `uv sync --frozen --all-extras` (reproducible env from `uv.lock`)
- installs the package in editable mode

If you’re not using `make`, the equivalent is:

```bash
uv sync --frozen --all-extras
uv run python -m pip install -e .
```

## Code style and standards

### Formatting & imports
- **Black** with **line length 100** (see `[tool.black]` in `pyproject.toml`)
- **isort** with Black profile and **line length 100**
- CI style checks are enforced via **pre-commit**

Run the repo’s style gate:

```bash
make check
```

### Docstrings
Use **NumPy-style** docstrings for public APIs and non-trivial internals.

### Avoiding “generated-file drift” (important)
If you touch a file that is generated by `codegen/` or the build hook, prefer:
1) finding the source-of-truth input in `codegen/`,
2) updating that input,
3) regenerating/bumping in the standard workflow.

## Testing

The repo uses `pytest`.

- Fast sanity check:

```bash
make smoketest
```

- Full test suite (with coverage):

```bash
make test
```

Tips for good tests in this repo:
- Favor **unit tests** by default; gate truly external dependencies behind markers.
- Use existing fixtures in `tests/conftest.py` and `tests/serverless/conftest.py`.
- Keep test data in `tests/test_data/` (or `tests/serverless/test_data/` when appropriate).
- The tests require an installation of the flagship package through a Docker image followed by
  both running the image as a container(service setup) and extracting the required files from the image (serverless setup).
  Check CI configuration under .github/workflows/ci_cd.yml and existing tests for examples.

### Running the pytest suite locally (reproducing CI)

The CI pipeline in `.github/workflows/tests.yml` runs the full pytest suite using Docker.
You can reproduce this locally without waiting for GitHub Actions. The steps below mirror
exactly what CI does.

#### Prerequisites

| Requirement | Why | Check |
|---|---|---|
| **Docker Desktop** (running) | The test fixtures spin up containers from the ADR Docker image | `docker info` |
| **GHCR authentication** | The image `ghcr.io/ansys-internal/adr_dev` is in a private registry | `docker login ghcr.io` |
| **`uv`** | Dependency management & virtualenv | `uv --version` |
| **`make`** | Wraps canonical workflows | `make --version` |
| **Python 3.10–3.12** | Required by the project | `python --version` |

#### 1. Install the project

```bash
make install
```

This runs `uv sync --frozen --all-extras` and installs the package in editable mode.

#### 2. Authenticate with GHCR and pull the Docker image

```bash
# One-time login (use a GitHub PAT with read:packages scope)
docker login ghcr.io -u <YOUR_GITHUB_USERNAME>

# Pull the ADR Docker image
make pull-docker
```

This pulls `ghcr.io/ansys-internal/adr_dev:latest` (~3 GB compressed, ~11 GB on disk).
If the image is already local, it will be reused automatically by the test fixtures.

#### 3. Set environment variables

The test suite has **two categories** of tests, each with different env var requirements:

| Test category | Directory | Env vars needed | What happens |
|---|---|---|---|
| **Serverless tests** | `tests/serverless/` | *None required* | Fixture creates a temporary Docker container, copies the ADR installation to a local temp dir, then tears the container down. Tests run against local SQLite DBs. |
| **Service-mode tests** | `tests/test_service.py`, `tests/test_report.py`, etc. | `ANSYSLMD_LICENSE_FILE` | Fixture starts a Docker container running the full ADR server (requires an Ansys license). |

**Serverless tests (no license needed):**

```bash
# No env vars required — just run:
uv run python -m pytest tests/serverless/ -v
```

**Full test suite (license required):**

```bash
# Set the license server (same value CI uses from the LICENSE_SERVER secret)
# On Windows (PowerShell):
$env:ANSYSLMD_LICENSE_FILE = "1055@your-license-server"
$env:ANSYS_DPF_ACCEPT_LA = "Y"

# On Linux/macOS:
export ANSYSLMD_LICENSE_FILE="1055@your-license-server"
export ANSYS_DPF_ACCEPT_LA="Y"
```

#### 4. Run the tests

**Full suite with coverage (mirrors `make test`):**

```bash
make test
```

Under the hood this runs:

```bash
uv run python -m pip install -e .[test]
uv run python -m pytest \
    -rvx --setup-show \
    --cov=ansys.dynamicreporting.core \
    --cov-report html:coverage-html \
    --cov-report term \
    --cov-report xml:coverage.xml
```

**Serverless tests only (fastest, no license):**

```bash
uv run python -m pytest tests/serverless/ -v
```

**Specific test file or test:**

```bash
uv run python -m pytest tests/serverless/test_adr.py -v
uv run python -m pytest tests/serverless/test_adr.py::test_create_string -v
```

**Skip tests that need DPF (no DPF server locally):**

```bash
uv run python -m pytest --ignore=tests/test_enhanced_images.py -v
```

#### 5. What the fixtures do (understanding the flow)

The test fixtures in `conftest.py` orchestrate the Docker ↔ test interaction:

- **`tests/serverless/conftest.py`** (`adr_init` / `adr_serverless` fixtures):
  1. Creates a `DockerLauncher`, pulls the image, creates a container.
  2. Copies `/Nexus/ADR` (or legacy `/Nexus/CEI`) from the container to a local temp directory.
  3. Tears down the container.
  4. Configures Django with SQLite databases from `tests/serverless/test_data/`.
  5. Runs `ADR.setup()` (migrations, static file collection).
  6. After all tests: calls `ADR.close()`.

- **`tests/conftest.py`** (`adr_service_create` / `adr_service_query` fixtures):
  1. Creates a `Service` with `ansys_installation="docker"`.
  2. Starts the Docker container running the full ADR web server on a random port.
  3. Requires `ANSYSLMD_LICENSE_FILE` in the environment (the container reads it).
  4. After tests: stops the service and container.

#### 6. Performance notes

- **First run** takes 3–5 minutes for the serverless tests because the Docker container
  must be created and the installation files (~3 GB) are copied out. Subsequent runs
  reuse the cached image.
- **Service-mode tests** are slower because they start/stop full ADR server containers.
- To speed up iteration, run only the specific test file or test you're working on.

#### 7. Troubleshooting

| Problem | Solution |
|---|---|
| `Can't initialize Docker` | Ensure Docker Desktop is running |
| `Can't pull Docker image` | Run `docker login ghcr.io` with a PAT that has `read:packages` scope |
| `ANSYSLMD_LICENSE_FILE` KeyError | Set the env var (only needed for service-mode tests) |
| `ADR has already been configured` | The ADR singleton persists across tests in the same process. This is expected — the session-scoped fixture handles init/teardown |
| Slow first run | The Docker image is ~11 GB on disk; the initial copy-from-container step is I/O heavy |
| Port conflicts | Fixtures use random ports (8000–11999). If you have other services on those ports, you may see conflicts |

## Branch conventions

Direct commits to `main` aren’t allowed.

Recommended prefixes (matches `CONTRIBUTING.md`):
- `fix/*` – bug fixes and small patches
- `feat/*` – new features
- `doc/*` – documentation-only changes
- `maint/*` – maintenance/CI
- `testing/*` – test changes
- `junk/*` – experimental work
- `no-ci/*` – low-impact work that shouldn’t trigger CI
- `release/*` – release-related work

## Key dependencies (source of truth)

The authoritative dependency list is in `pyproject.toml`.
Highlights:
- `requests`, `docker`, `Pillow`
- Serverless stack: `django`, `djangorestframework`, `django-guardian`, `psycopg[binary]`
- Data/analytics: `numpy`, `pandas`, `statsmodels`
- Export/rendering: `python-pptx`, `weasyprint`, `django-weasyprint`

## Making changes (workflow)

### 1) Triage & understand
- Find the **public surface** impacted (imports, docs, examples).
- Search for existing helpers and patterns before adding new ones.
- Identify whether the change is **core**, **serverless**, **docker**, or **docs**.

### 2) Implement minimally
- Keep changes local.
- Don’t refactor adjacent code unless needed for correctness.
- Do not touch or refactor or reformat unrelated code, unless explicitly part of the change or explicitly asked.

### 3) Add tests
- Look at .github/workflows/ci_cd.yml and Makefile for the test flow.
- The flow requires several steps to reproduce and run locally and is not a direct pytest run.
- Add a regression test for bugs.
- For features, cover the main path plus a failure/edge case. Code coverage must be 90%.
- Look at existing tests for patterns, mainly in `tests/serverless/`.
- Tests should not be forced to increase coverage artificially; focus on meaningful coverage.
- Use existing fixtures where possible.

### 4) Run quality gates (expected before finishing)

- Run all checks locally and fix failures before pushing:

```bash
make check
make smoketest
make test
```

## Common code patterns

### Imports
Keep imports grouped as:
1) standard library
2) third-party
3) local package imports

### Error handling
Prefer raising repo-specific exceptions where appropriate (search for existing exception types rather than inventing new ones).

### Docstring template (NumPy)
```python
def function_name(param1, param2):
    """Brief summary.

    Parameters
    ----------
    param1 : type
        Description.
    param2 : type
        Description.

    Returns
    -------
    type
        Description.
    """
```

## Docs

Build docs via:

```bash
make docs
```

Docs sources live under `doc/source/`.

## CI/CD notes (what agents should assume)

GitHub Actions jobs typically enforce:
- formatting / pre-commit
- unit tests
- smoke tests
- docs build (depending on workflow)

If you change:
- `pyproject.toml` or dependency groups → expect lockfile checks (`uv lock --locked`) to matter.
- Check out README for instructions on setting up the development environment.

## Repo-specific notes

### Service mode
- Source: `src/ansys/dynamicreporting/core/adr_service.py` and related modules.
- Backed by REST API calls to Ansys Dynamic Reporting service.

### Utilities
- Source: `src/ansys/dynamicreporting/core/utils/`
- Common helpers for file handling, data conversion, REST interactions, etc.

### Serverless mode
- Source: `src/ansys/dynamicreporting/core/serverless/`
- Backed by Django models + database (often PostgreSQL via psycopg)

### Docker support
- Source: `src/ansys/dynamicreporting/core/docker_support.py`
- Convenience script: `scripts/pull_adr_image.sh`

### Code generation
- Helpers live in `codegen/`
- Some core modules may be generated during build hooks; avoid editing outputs directly unless unavoidable.

### Release tooling (FYI)
- The repo uses tag-driven versioning through Hatch (`uv run hatch version`).
- See `scripts/tag_release.sh` and `make tag`.
- Releases are automated via GitHub Actions workflows.
- Never make releases by hand.
- Never do anything release-related, unless explicitly told to.

## Getting help

- Issues: https://github.com/ansys/pydynamicreporting/issues
- Discussions: https://discuss.ansys.com/
- Email: pyansys.core@ansys.com

---

**Last updated**: February 19, 2026
**Maintained by**: ANSYS, Inc. and Ansys ADR Team
